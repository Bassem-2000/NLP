{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset,\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "8YiS5_jGuRcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(train_data, train_label), (test_data,test_label) = imdb.load_data(num_words=10000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6EdrvJ4v1-Fd",
        "outputId": "45354183-f9e4-4a42-85b4-b6efba9a861d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = imdb.get_word_index()\n",
        "reserve_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "decoded_review = ' '.join(reserve_word_index.get(i-3, '?') for i in train_data[0])\n",
        "\n",
        "len(word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7W5wew34y-U",
        "outputId": "b84f32b8-f937-483c-f046-bfa9d22dd271"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "\u001b[1m1641221/1641221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "88584"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_sequences(sequences, dim=10000):\n",
        "  vectors = np.zeros((len(sequences), dim), dtype=np.float32)\n",
        "  for i, sequence in enumerate(sequences):\n",
        "    vectors[i, sequence] = 1.0\n",
        "  return vectors\n"
      ],
      "metadata": {
        "id": "_dTx3X_j-Hq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = vectorize_sequences(train_data)\n",
        "y_train = np.asarray(train_label).astype('float32')\n",
        "x_test = vectorize_sequences(test_data)\n",
        "y_test = np.asarray(test_label).astype('float32')"
      ],
      "metadata": {
        "id": "_moo3hUDFxoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = torch.tensor(x_train)\n",
        "y_train = torch.tensor(y_train).unsqueeze(1)\n",
        "x_test = torch.tensor(x_test)\n",
        "y_test = torch.tensor(y_test).unsqueeze(1)"
      ],
      "metadata": {
        "id": "dXhtxyNGFxgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_val = x_train[:10000]\n",
        "y_val = y_train[:10000]\n",
        "x_train = x_train[10000:]\n",
        "y_train = y_train[10000:]"
      ],
      "metadata": {
        "id": "nq7iCPwTMyXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset =  TensorDataset(x_train, y_train)\n",
        "val_dataset = TensorDataset(x_val, y_val)\n",
        "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=512)"
      ],
      "metadata": {
        "id": "Y6UyqCsNn4JW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLC_beBNthkG",
        "outputId": "ad1c7d17-4575-4c72-a2fc-4880b7bad013"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([15000, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BinaryClassificationModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(BinaryClassificationModel, self).__init__()\n",
        "    self.fc1 = nn.Linear(10000, 16)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.fc2 = nn.Linear(16, 16)\n",
        "    self.fc3 = nn.Linear(16, 1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.fc1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.fc2(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.fc3(x)\n",
        "    x = self.sigmoid(x)\n",
        "    return x\n",
        "\n",
        "model = BinaryClassificationModel()\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.RMSprop(model.parameters(), lr=0.001)\n",
        "epochs = 20\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  model.train()\n",
        "  running_loss = 0.0\n",
        "  correct_train = 0\n",
        "  total_train = 0\n",
        "\n",
        "  for inputs, labels in train_loader:\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "\n",
        "    preds = (outputs >= 0.5).float()\n",
        "    correct_train += (preds == labels).sum().item()\n",
        "    total_train += labels.size(0)\n",
        "\n",
        "  train_accuracy = correct_train / total_train\n",
        "\n",
        "  model.eval()\n",
        "  val_loss = 0.0\n",
        "  correct_val = 0\n",
        "  total_val = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for val_inputs, val_labels in val_loader:\n",
        "      val_outputs = model(val_inputs)\n",
        "      val_loss += criterion(val_outputs, val_labels).item()\n",
        "\n",
        "      val_preds = (val_outputs >= 0.5).float()\n",
        "      correct_val += (val_preds == val_labels).sum().item()\n",
        "      total_val += val_labels.size(0)\n",
        "\n",
        "  val_accuracy = correct_val / total_val\n",
        "  print(f\"Epoch {epoch+1}/{epochs} - Loss: {running_loss:.4f} - Acc: {train_accuracy:.4f} - Val Loss: {val_loss:.4f} - Val Acc: {val_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gqPayKB8Ba5",
        "outputId": "b3ec81e0-3641-49f3-fc6c-fde43fdecdf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Loss: 14.7354 - Acc: 0.7891 - Val Loss: 6.9505 - Val Acc: 0.8801\n",
            "Epoch 2/20 - Loss: 7.7982 - Acc: 0.9159 - Val Loss: 5.6507 - Val Acc: 0.8917\n",
            "Epoch 3/20 - Loss: 5.5606 - Acc: 0.9394 - Val Loss: 5.4555 - Val Acc: 0.8901\n",
            "Epoch 4/20 - Loss: 4.3614 - Acc: 0.9547 - Val Loss: 5.5772 - Val Acc: 0.8867\n",
            "Epoch 5/20 - Loss: 3.5588 - Acc: 0.9645 - Val Loss: 5.7255 - Val Acc: 0.8892\n",
            "Epoch 6/20 - Loss: 2.8751 - Acc: 0.9739 - Val Loss: 7.0223 - Val Acc: 0.8719\n",
            "Epoch 7/20 - Loss: 2.3807 - Acc: 0.9800 - Val Loss: 6.3866 - Val Acc: 0.8833\n",
            "Epoch 8/20 - Loss: 1.9453 - Acc: 0.9848 - Val Loss: 6.8828 - Val Acc: 0.8811\n",
            "Epoch 9/20 - Loss: 1.5212 - Acc: 0.9895 - Val Loss: 7.7630 - Val Acc: 0.8781\n",
            "Epoch 10/20 - Loss: 1.3105 - Acc: 0.9911 - Val Loss: 8.0775 - Val Acc: 0.8754\n",
            "Epoch 11/20 - Loss: 0.9671 - Acc: 0.9947 - Val Loss: 8.5154 - Val Acc: 0.8765\n",
            "Epoch 12/20 - Loss: 0.7633 - Acc: 0.9968 - Val Loss: 9.7912 - Val Acc: 0.8690\n",
            "Epoch 13/20 - Loss: 0.6054 - Acc: 0.9983 - Val Loss: 9.9145 - Val Acc: 0.8744\n",
            "Epoch 14/20 - Loss: 0.4522 - Acc: 0.9990 - Val Loss: 10.4917 - Val Acc: 0.8730\n",
            "Epoch 15/20 - Loss: 0.3420 - Acc: 0.9995 - Val Loss: 11.4966 - Val Acc: 0.8731\n",
            "Epoch 16/20 - Loss: 0.2667 - Acc: 0.9998 - Val Loss: 12.1057 - Val Acc: 0.8701\n",
            "Epoch 17/20 - Loss: 0.1994 - Acc: 0.9999 - Val Loss: 12.9704 - Val Acc: 0.8709\n",
            "Epoch 18/20 - Loss: 0.1540 - Acc: 0.9999 - Val Loss: 13.5176 - Val Acc: 0.8714\n",
            "Epoch 19/20 - Loss: 0.1214 - Acc: 0.9999 - Val Loss: 14.0797 - Val Acc: 0.8702\n",
            "Epoch 20/20 - Loss: 0.0960 - Acc: 0.9999 - Val Loss: 15.0986 - Val Acc: 0.8689\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SljBlWTpyzAr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}